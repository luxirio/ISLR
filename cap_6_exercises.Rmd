---
title: "Cap_6_Exercises"
author: "Gustavo_Sousa"
date: "2022-07-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a notebook of the Introduction to Statistical Learning (ISLR) - Capitule 6 (Model Selection and regularization)

## 6.6 Exercises

### Conceptual

1)  We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2,..., p predictors. Explain your answers:

```{=html}
<!-- -->
```
a)  Which of the three models with k predictors has the smallest training RSS? Probably the best subset has the lowest training RSS, because it's method searches through a bigger picture/space than p + 1 models and can include variables that would have been excluded from the analysis. (Gustavo)

Answer: Best subset selection has the lowest training RSS as it fits models for every possible combination of predictors. When p is very large, this increases the chance of finding models that fit the training data very well.

b)  Which of the three models with k predictors has the smallest test RSS? Probably the stepwise approaches. Because as the best subset approach better probably is going to yield a better fit for the training set, it also simulteanously icresases variance. (Gustavo)

Answer: Best test RSS could be provided by any of the models. Bet subset considers more models than the other two, and the best model found on the training set could also be the best one for a test set. Forward and backward stepwise consider a lot fewer models, but might find a model that fits the test set very well as it tends to avoid over fitting when compared to best subset.

(c) True or False:

```{=html}
<!-- -->
```
i.  The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection. FALSE

    Answer: TRUE

ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection. TRUE

iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection. FALSE

iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection. FALSE

v.  The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.

2\. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.

(a) The lasso, relative to least squares, is:

i\. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

ii\. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

iii\. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

iv\. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Answers:

a)

Lasso:

(iii) Less flexible and will give improved prediction accuracy when
its increase in bias is less than its decrease in variance. As lambda
increases, flexibility of fit decreases, and so the estimated coefficients decrease with some being zero. This leads to a substantial decrease in the variance of the predictions for a small increase in bias.

b)

Ridge:

(iii) Less flexible and will give improved prediction when its increase in bias is less than its decrease in variance. Flexibility of fit decreases as coefficients approaches zero. This can lead to a decrease in the variance of the prediction with a small increase in bias.

c)

Linear methods:

(ii) More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

3\. Suppose we estimate the regression coefficients in a linear regression model by minimizing for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.

![](images/paste-E53D9205.png){width="584"}

(a) As we increase s from 0, the training RSS will:

i\. Increase initially, and then eventually start decreasing in an inverted U shape.

ii\. Decrease initially, and then eventually start increasing in a U shape.

iii\. Steadily increase.

iv\. Steadily decrease.

v\. Remain constant.

i\. Increase initially, and then eventually start decreasing in an inverted U shape.

Answer:  - (i) Decrease steadily. As s increases the
constraint on beta decreases and the RSS reduces until we reach the
least squares answer.

(b) Repeat (a) for test RSS.

ii\. Decrease initially, and then eventually start increasing in a U shape (True)

(c) Repeat (a) for variance.

iii\. Steadily increase.

(d) Repeat (a) for (squared) bias.

iv\. Steadily decrease.

(e) Repeat (a) for the irreducible error.

v\. Remain constant.

4\. Suppose we estimate the regression coefficients in a linear regression

model by minimizing

![](images/paste-9E19B468.png)

for a particular value of λ. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.

i\. Increase initially, and then eventually start decreasing in an inverted U shape.

ii\. Decrease initially, and then eventually start increasing in a U shape.

iii\. Steadily increase.

iv\. Steadily decrease.

v\. Remain constant.

(a) As we increase λ from 0, the training RSS will:

iii\. Steadily increase.

(b) Repeat (a) for test RSS.

ii\. Decrease initially, and then eventually start increasing in a U shape.

(c) Repeat (a) for variance.

iv\. Steadily decrease.

(d) Repeat (a) for (squared) bias.

iii\. Steadily increase.

(e) Repeat (a) for the irreducible error.

v\. Remain constant.

6\. We will now explore (6.12) and (6.13) further.

(a) Consider (6.12) with p = 1. For some choice of y1 and λ \> 0, plot (6.12) as a function of β1. Your plot should confirm that (6.12) is solved by (6.14).

```{r}
y = 10
beta = seq(-10, 10, 0.1)
lambda = 5

eqn1 = (y - beta)^2 + lambda*(beta)^2
which.min(eqn1)

estimated_beta = y/(1 + lambda)
estimated_value = (y - estimated_beta)^2 + lambda*(estimated_beta)^2

plot(beta, eqn1, main="Ridge Regression Optimization", xlab="beta", ylab="Ridge Eqn output",type="l")
points(beta[118],eqn1[118],col="red", pch=24,type = "p")
points(estimated_beta, estimated_value,col="blue",pch=20,type ="p")
```

(b) Consider (6.13) with p = 1. For some choice of y1 and λ \> 0, plot (6.13) as a function of β1. Your plot should confirm that (6.13) is solved by (6.15).

```{r}
y = 10
beta = seq(-10, 10, 0.1)
lambda = 5

eqn2 = (y - beta)^2 + lambda*(abs(beta))

estimated_beta2 = y - lambda/2
estimated_value = (y - estimated_beta2)^2 + lambda*(estimated_beta2)

plot(beta, eqn2, main = "Lasso Optimization", xlab = "beta", ylab = "Eqn 6.13")
points(estimated_beta2, estimated_value,col="red",pch=20,type ="p")
```
