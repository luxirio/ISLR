---
title: "Lab_Tree"
author: "Gustavo"
date: "2022-09-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## *Fitting Classification Trees*

Importing the necessary libraries

```{r}
library(tree)
library(ISLR2)
attach(Carseats)
# We create a factor dividing into "Yes" or "No" Sales > 8 or not, respectively
high_factor <- factor(ifelse(Sales <= 8, "No", "Yes"))
```

Importing the dataset and transforming it into a dataframe, as it is needed by the tree package functions.

```{r}
# Combining the factor created before into the dataframe
Carseats_df <- data.frame(Carseats, high_factor)

# Double checking if it is a dataframe
is.data.frame(Carseats_df)
```

Deviating a little bit from the book, we're going to explore a little bit the dataset, as it is more adequate as sequential analysis rather than going directly into modelling.

```{r}
print("The dataset has the current shape (lines x columns):")
dim(Carseats_df)
```

```{r}
print("Performing a summary of the dataset:")
summary(Carseats_df)
```

Searching a little bit further into the dataset, it is simulated data with 400 observations containing the sales of child car seats at 400 different stores.

More about the variables, gathered from [here](https://rdrr.io/cran/ISLR/man/Carseats.html "here"):

-   **Sales**: Unit sales (thousands) at each location

-   **CompPrice**: Price charged by store at each location

-   **Income**: Community income level (in thousand of dollars)

-   **Advertising**: Local advertising budget for company at each location (thousand of dollars)

-   **Population**: Pop size in region (in thousands)

-   **Price**: Price company charges for car seats at each store

-   **ShelveLoc**: A factor with levels (Bad/Good/Medium) indicating the quality of the shelving location for the car seats at each site

-   **Age**: Average age of local population

-   **Education**: Education level of local population

-   **Urban**: Yes or No to indicate whether the store is urbal or rural location

-   **US**: A factor containing Yes or No to indicate the store is in the US or not.

-   high_factor: Feature engineered as a factor to indicate whether the store has more

This data set is comprised of 12 variables including

We now use the `tree()` function to fit a classification tree in order to predict high_factor variables minus Sales. The synthax of `tree()` function is quite similar to that of `lm()` function

```{r}
tree.carseats <- tree(high_factor ~. - Sales, Carseats_df)
```

The `summary()` function returns to us the variables that are used in the tree, the training error rate and other useful statistical infos aswell.

```{r}
print("Information about the classification tree built")
summary(tree.carseats)
```

One of the greatest properties of tree-based models is that it can be displayed visually. The `plot()` function display the structure and the `text()` function can display the node lables.

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

Analysing the tree, the most important indicator of Sales appears to be the shelving location, since the first branch differentiates Good locations from bad and medium locations.

> *If we just type the name of the tree object, R prints output corresponding to each branch of the tree. R displays the split criterion (e.g. Price \< 92.5), the number of observations in that branch, the deviance, the overall prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No. Branches that lead to terminal nodes are indicated using asterisks.*

Typing the name of the tree object returns the corresponding to each branch of the tree. For example:

```{r}
tree.carseats
```

Branches leading to terminal nodes are indicated using asterisks (**\***).

To evaluate the performance properly on these data we must estimate the test error rather than simply compute the training error. Finally, using the `predict()` function, we can predict new observations.

```{r}
set.seed(2) # Copying the results of the textbook

# Sampling 200 observations as the training sample
# This returns a list of numbers and we can use it as index to select the training and test group
train <- sample(1:nrow(Carseats_df), 200)
#Selecting the test group
Carseats.test <- Carseats_df[-train,]
high_factor.test <- high_factor[-train]

#Building the model
tree.carseats <- tree(high_factor ~. - Sales, Carseats_df, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")

table(tree.pred, high_factor.test)

cat("The accuracy (Sum of True Negative (TN) and True Positive (TP)) is:", (104+50)/200)
```

Next we can see whether the pruning of the tree might lead to improvement of the results. For this, we can use the function `cv.tree()` to determine the best level of tree complexity. We can use the argument **FUN = prune.misclass** in order to indicate whether we want the classification error rate to guide the cross-validation process rather the "deviance", which is the default.

```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)

cv.carseats
```

We can see graphically the cross-validation results:

```{r}
par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")

```

Now that we know that the best subset tree is a tree with 9 nodes, as we can see above with the dev x size plot (the elbow is at the point 9), we can apply the `prune.missclass()` function to prune the tree to the **nine node tree**.

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

So we made a cleaner or more pruned tree, which is in resonance with the Ockham's razor principle. But how well does it perform?

```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, high_factor.test)

cat("The TN + TP proportion from the test group is:", ((97+58)/200) )
```

So we've made a slightly better accuracy tree but the tree is a lot cleaner, which is preferable, i.e. is better to have a model with fewer variables and more interpretable than a model with a lot of variables (and nodes).

## *Fitting Regression Trees*

Now we perform some quantitative analysis on the Boston data set. We begin by separating the test and the training group and fit the tree to the training data. A little bit about the dataset can be found [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)

There are 14 attributes/variables in this data set, their descriptions can be found below:

1.  **crim** - per capita crime rate by town

2.  **zn** - proportion of residential land zoned for lots over 25,000 sq.ft.

3.  **indus** - proportion of non-retail business acres per town.

4.  **chas** - Charles River dummy variable (1 if tract bounds river; 0 otherwise)

5.  **nox**- nitric oxides concentration (parts per 10 million)

6.  **rm** - average number of rooms per dwelling

7.  **age** - proportion of owner-occupied units built prior to 1940

8.  **dis** - weighted distances to five Boston employment centres

9.  **rad** - index of accessibility to radial highways

10. **tax** - full-value property-tax rate per \$10,000

11. **ptratio**- pupil-teacher ratio by town

12. **b** - 1000(Bk - 0.63)\^2 where Bk is the proportion of blacks by town

13. **lstat**- % lower status of the population

14. **medv**- Median value of owner-occupied homes in \$1000's

Basic information about the data set can be found in the code chunk below, to maintain good practices:

```{r}

dim(Boston)
summary(Boston)
```

Fitting the regression, we can see:

```{r}
# Setting the seed in order to make the results reproducible
set.seed(1)

# Separating the train and test set
train <- sample(1:nrow(Boston), nrow(Boston)/2)

# Building the model
tree.boston <- tree(medv ~., Boston, subset = train)

#Summary of the model
summary(tree.boston)
```

The only variables used to build the model were rm, lstat, crim and age, only 4 of the 12 predictor variables. Plotting the tree to get a more visual feeling:

```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

We can infer from the tree, that houses with **rm** \> 7.553 are more valuable and the model predicts a mean of \$45.38 for houses that has more than 7-8 rooms.

Conclusion from the tree printed above:

**Smaller** houses, with **lstat** \>= 14.405, **crim** \> 11.48 and **age** \>= 93.95 are worth way less. That is, if a **smaller** house, with **lower economic status**, **high criminality** and **old**, they are **worth way less**.

Now we perform cross-validation. We could do this step right from the beggining, of course:

```{r}

# Remember that tree.boston is with the training subset
cv.boston <- cv.tree(tree.boston)

# Plotting the results
plot(cv.boston$size, cv.boston$dev, type = "b")
```

So the tree with 7 nodes can be selected but arguably the tree with 3 nodes could be selected as it looks like an elbow. We can prune the tree using the following:

```{r}
prune.boston <- prune.tree(tree.boston, best = 3)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

Well, the tree above is way cleaner and way easier to interpret. The interpretation logic is the same as the one before, that is:

Houses with more rooms (rm \>= 6.95) are the **most expensive**. **Smaller** and **lower** **status** houses (lstat \< 14.405) are the **cheapest ones**.

Keeping up with the cross-validation results, the unpruned tree should make the best predictions on the test set.

```{r}
boston.pred <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train, "medv"]
plot(boston.pred, boston.test)
abline(0,1)

mean((boston.pred - boston.test)^2)

```

The model is doing fine, and the graph looks like an stairway. The MSE is 35.29 and the RMSE is 5.941, indicating that the model leads to test predictions on average within approximately 5.941 of the true median home value for this data set. PRETTY GOOD!

## Bagging and Random Forests (RFs)

Now we begin to apply some advanced methods into our data!

Remembering that bagging is a special case of random forest where m = p. We can perform the bagging method using mtry = 12.

```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~., data = Boston, subset = train, mtry = 12, importance = TRUE)

```

> The argument mtry = 12 indicates that all 12 predictors should be considered for each split of the tree, which is actually bagging.

Let's see how well the bagging model performs on the data:

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
```

Printing out the MSE of the Random Forest model above:

```{r}
mean((yhat.bag - boston.test)^2)
```

So the test MSE associated with the bagged regression 23.419, against the 35.29 for the pruned single tree.

We can also specify the number of tree and compare:

```{r}
#notice that it is mtry = 12 so it is still bagging

bag.boston <- randomForest(medv~., data = Boston, subset = train, 
                           mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
```

Evaluating the RF model (which is a bagging model as mtry = 12) and using ntree = 25:

```{r}
mean((yhat.bag - boston.test)^2)
```

So now we have 25.750, and that is a little higher than the defalt that before. Now let's try using mtry with a value less than the total variables (mtry = 6).

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
```

Evaluating the RF model with mtry = 6 we get a MSE of....

```{r}
mean((yhat.rf - boston.test)^2)
```

20! That's even better. Let's plot it and compare it with the plot before:

```{r}
par(mfrow = c(1,2))
plot(yhat.bag, boston.test, main = "BAGGING")
abline(0,1)
plot(yhat.rf, boston.test, main = "RANDOM FOREST")
abline(0,1)

```

We can also plot the variable importance to estimate the response variable using the function `varImpPlot()`:

```{r}
varImpPlot(rf.boston)
```

> The results indicate that across all of the trees considered in the random forest, the wealth of the community (lstat) and the house size (rm) are by far the two most important variables.

## Boosting

The authors use the gbm package `gbm()` function to fit boosted regression trees to the Boston data set. We use the option for the argument `distribution = "gaussian"` since it is a regression problem but if it were a classification problem we would use the argument "`bernoulli"` . The argument `n.trees` limits the quantity of trees and the `interaction.depth` limits the depth of each tree.

```{r}
library(gbm)
set.seed(1)

boost.boston <- gbm(medv~., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
```

We can use the `summary()` function to produce a relative influence plot that also outputs the relative influence statistics of each variable accompanied by a plot:

```{r}
summary(boost.boston)
```

We see that rm and lstat are the most important variables. We can produce the partial depence plots for these two variables, for example:

```{r}
par(mfrow = c(1,2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

In this case the response variable y increases as rm increases and decreases as lstat increases. This makes total sense when we see what variable relates to the median house prices.

We now use the boosted model to predict the test group:

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
```

Evaluating the MSE of the model relative to the test group:

```{r}
mean((yhat.boost - boston.test)^2)
```

So we came from roughly 35 from the single pruned tree to 20 with random forest and 18 with the boosting; We can go further, we can use a shrikage parameter lambda of 0.2, while the default value is 0.001.

```{r}
boost.boston <- gbm(medv ~., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)

yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)

mean((yhat.boost - boston.test)^2)
```

We roughly have cut in half our predictions!! Let's see how that changes graphically:

```{r}
par(mfrow = c(1,3))
plot(yhat.bag, boston.test, main = "BAGGING")
abline(0,1)

plot(yhat.rf,boston.test, main = "RF")
abline(0,1)

plot(yhat.boost, boston.test, main = "BOOSTED")
abline(0,1)
```

## Bayesian Additive Regression Trees

Now testing Bayesian Additive Regression Trees:

```{r}
library(BART)

x <- Boston[, 1:12]
y <- Boston[, "medv"]

xtrain <- x[train,]
ytrain <- y[train]

xtest <- x[-train,]
ytest <- y[-train]

set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```

```{r}
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```

Let's plot all against each other:

```{r}
par(mfrow = c(1,4))

plot(yhat.bag, boston.test, main = "BAGGING")
abline(0,1)

plot(yhat.rf,boston.test, main = "RF")
abline(0,1)

plot(yhat.boost, boston.test, main = "BOOSTED")
abline(0,1)

plot(yhat.bart, boston.test, main = "BART")
abline(0,1)

```

FINISH!
