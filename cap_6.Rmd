---
title: "Cap_6"
author: "Gustavo"
date: '2022-07-08'
output: html_document
---

### Subset Selection Methods (Best Subset Selection)

```{r loading}
library(ISLR2)

attach((Hitters))
names(Hitters)

```

```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
```

Sanity Check is there any NA?

```{r}
sum(is.na(Hitters))
```

Of course it is 0 as we did remove NAs using `na.omit()` function

```{r}
library(leaps) 
subset_fit_partial <-regsubsets(Salary ~., Hitters)
summary(subset_fit_partial)
```

\`An asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.

The function goes only up untill the 8th model. If we want to include the 19 variables we could do:

```{r}
subset_fit_full <- regsubsets(Salary~., data = Hitters, nvmax = 19)
summary(subset_fit_full)
```

```{r}
(summary(subset_fit_full)$rsq)
```

We can now plot the R² of each subset model, by plotting the `summary(subset_fit_full)$rsq`

```{r}
plot(summary(subset_fit_full)$rsq, xlab = "Number of variables",
     ylab = "RSS")
```

### Plotting the Adjusted "R(2)":

```{r}
plot(summary(subset_fit_full)$adjr2, xlab = "Number of variables",
     ylab = "Adj R²", type = "l")

points(11, summary(subset_fit_full)$adjr2[11], col = "red", pch = 20)
```

### Plotting the "Cp" Metric:

```{r}
plot(summary(subset_fit_full)$cp, xlab = "Number of Variables", ylab = "Cp",
     type = "l")
which.min(summary(subset_fit_full)$cp)

points(10, summary(subset_fit_full)$cp[10], col = "red", cex = 2, pch = 20)
```

### Plotting the BIC Metric:

```{r}
which.min(summary(subset_fit_full)$bic)

plot(summary(subset_fit_full)$bic, xlab = "Number of variables",
     ylab = "BIC")
points(6, summary(subset_fit_full)$bic[6], col = "red", pch = 20)

```

### The top row of each plot contains a black square for each variable selected

According to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to −150. However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.

```{r}
plot(subset_fit_full, scale = "r2")
plot(subset_fit_full, scale = "adjr2")
plot(subset_fit_full, scale = "Cp")
plot(subset_fit_full, scale = "bic")

coef(subset_fit_full, 6)
```

### Forward and Backward Step wise Selection

```{r}

fit_forward <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(fit_forward)

fit_backward <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
```

```{r}
print("Best subset 7")
coef(subset_fit_full, 7)

print("Best foward")
coef(fit_forward, 7)

print("Best backward")
coef(fit_backward, 7)
```

In order to use the validation set approach, we begin by splitting the observations into a training set and a test set. We do this by creating a random vector, train, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. The vector test has a TRUE if the observation is in the test set, and a FALSE otherwise. Note the ! in the command to create test causes TRUE to be switched to FALSE and viceversa. We also set a random seed so that the user will obtain the same training set/test set split.

```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test <- (!train)
```

We fit the model and predict

```{r}
regfit_best <- regsubsets(Salary~., data = Hitters[train,],
                          nvmax = 19)
test.mat <- model.matrix(Salary~., data = Hitters[test,])
```

We use a for to calculate the errors between each best subset model:

```{r}
val.errors <- rep (NA, 19)

for (i in 1:19) {
  coefi <- coef(regfit_best, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
  
}
```

```{r}
which.min (val.errors)
```

The best model (using the minimum value errors) contains 7 variables and the coefficients to each variable are depicted below:

```{r}
coef(regfit_best, 7)
```

```{r}
predict_regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}


```

```{r}
k <- 10
n <- nrow(Hitters)
set.seed(1)

folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))


```

```{r}
for (j in 1:k){
  best.fit <- regsubsets(Salary ~.,
                         data = Hitters[folds != j, ], nvmax = 1)
  for (i in 1:19){
    pred <- predict(best.fit, Hitters[folds == j, ], id = i)
    cv.errors[j, i] <-
      mean((Hitters$Salary[folds == j] - pred)^2)
  }
}

```
